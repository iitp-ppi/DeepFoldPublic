import argparse
import gzip
import json
import logging
import multiprocessing
import pickle
import shutil
import time
from collections import defaultdict
from pathlib import Path
from typing import Callable, Dict, List, Optional, Set, Tuple

import numpy as np
import pandas as pd
from tqdm.auto import tqdm

from deepfold.utils.log_utils import setup_logging

logger = logging.getLogger(__name__)

ID_TO_RESTYPE = {
    0: "A",
    1: "R",
    2: "N",
    3: "D",
    4: "C",
    5: "Q",
    6: "E",
    7: "G",
    8: "H",
    9: "I",
    10: "L",
    11: "K",
    12: "M",
    13: "F",
    14: "P",
    15: "S",
    16: "T",
    17: "W",
    18: "Y",
    19: "V",
    20: "X",
    21: "-",
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--parsed_assembly_dirpath",
        type=Path,
        required=True,
        help="Path to 'assembly' data directory. Generate it with `gather_assembly.py`.",
    )
    parser.add_argument(
        "--output_dirpath",
        type=Path,
        required=True,
        help="Path to output directory.",
    )
    parser.add_argument(
        "--chains_filepath",
        type=Path,
        required=True,
        help="Path to a TSV file generated by MMseqs2. Consider that identifiers for each sequence are label_asym_id.",
    )
    parser.add_argument(
        "--chain_clusters_filepath",
        type=Path,
        required=False,
        help="Path to a TSV file generated by MMseqs2 (with sequence identitiy 1.0). Consider that identifiers for each sequence are label_asym_id.",
    )
    parser.add_argument(
        "--pdb_obsolete_filepath",
        type=Path,
        required=True,
        help="Path to `obsolete.dat` file. Download it via 'ftp://ftp.wwpdb.org/pub/pdb/data/status/obsolete.dat'.",
    )
    parser.add_argument(
        "--num_parallel_processes",
        type=int,
        default=16,
        help="Num parallel processes used during preprocessing.",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Whether to override existing output files.",
    )
    args = parser.parse_args()
    return args


def load_chain_cluster(chain_cluster_filepath: Path) -> Dict[str, List[str]]:
    clusters = defaultdict(list)
    with open(chain_cluster_filepath) as fp:
        lines = fp.read().strip().split("\n")
    for line in lines:
        center, member = line.split()
        clusters[center].append(member)
    return dict(**clusters)


def create_chain_dict(
    parsed_chain_filepath: Path,
    cluster_center: str,
    cluster_size: int,
) -> pd.DataFrame:
    pdb_id = parsed_chain_filepath.parent.name.lower()
    chain_id = parsed_chain_filepath.name.split(".")[0].split("_")[1]
    pdb_chain_id = f"{pdb_id}_{chain_id}"

    assembly_json_path = parsed_chain_filepath.parent / f"{pdb_id.upper()}-1.json"
    with open(assembly_json_path, "r") as fp:
        assembly_data = json.load(fp)

    with gzip.open(parsed_chain_filepath, "rb") as fp:
        chain_dict = pickle.load(fp)

    assert pdb_id == assembly_data["entry_id"].lower()
    sequence = "".join(ID_TO_RESTYPE[x] for x in chain_dict["aatype"])

    chain = {
        "pdb_chain_id": pdb_chain_id,  # format: `{pdb_id}_{label_asym_id}`
        "pdb_id": pdb_id,
        "pdb_cluster_center": cluster_center,
        "pdb_cluster_size": cluster_size,
        "mmcif_chain_id": chain_id,
        "release_date": assembly_data["release_date"],
        "exptl_method": assembly_data["method"],
        "resolution": assembly_data["resolution"],
        "sequence_length": len(sequence),
        "sequence": sequence,
    }

    return chain


def apply_func_parallel(
    func: Callable,
    args_list: List[tuple],
    num_parallel_processes: int,
) -> list:
    if not isinstance(args_list, list):
        raise TypeError(f"args_list is of type {type(args_list)}, but it should be of type {list}")
    for args in args_list:
        if not isinstance(args, tuple):
            raise TypeError(f"args is of type {type(args)}, but it should be of type {tuple}")

    if num_parallel_processes > 0:
        async_results = []
        pool = multiprocessing.Pool(num_parallel_processes)
        for args in args_list:
            ar = pool.apply_async(func, args)
            async_results.append(ar)
        results = [ar.get() for ar in tqdm(async_results)]
        pool.close()
        pool.join()
    else:
        results = []
        for args in tqdm(args_list):
            r = func(*args)
            results.append(r)

    return results


def process_gathered_chains(
    parsed_assembly_dirpath: Path,
    output_dirpath: Path,
    chains_filepath: Path,
    pdb_obsolete_filepath: Path,
    num_parallel_processes: int,
    force: bool = True,
    chain_clusters_filepath: Path | None = None,
) -> None:
    logger.info("process_gathered_chains has started...")

    logger.info(f"parsed_assembly_dirpath={repr(parsed_assembly_dirpath)}")
    logger.info(f"output_dirpath={repr(output_dirpath)}")
    logger.info(f"chains_filepath={repr(chains_filepath)}")
    logger.info(f"pdb_obsolete_filepath={repr(pdb_obsolete_filepath)}")
    logger.info(f"num_parallel_processes={repr(num_parallel_processes)}")
    logger.info(f"force={repr(force)}")
    logger.info(f"chain_clusters_filepath={repr(chain_clusters_filepath)}")

    if not parsed_assembly_dirpath.exists():
        raise FileNotFoundError(f"{repr(parsed_assembly_dirpath)} does not exist!")
    if not chains_filepath.exists():
        raise FileNotFoundError(f"{repr(chains_filepath)} does not exist!")
    if not pdb_obsolete_filepath.exists():
        raise FileNotFoundError(f"{repr(pdb_obsolete_filepath)} does not exist!")
    if chain_clusters_filepath is not None and not chain_clusters_filepath.exists():
        raise FileNotFoundError(f"{repr(chain_clusters_filepath)} does not exist!")

    output_dirpath.mkdir(exist_ok=force, parents=True)

    # chain_dicts_dirpath = output_dirpath / "dicts"
    # chain_dicts_dirpath.mkdir(exist_ok=force)
    # logger.info(f"Chain dicts will be saved to {repr(chain_dicts_dirpath)}")

    center_ids = list(load_chain_cluster(chains_filepath).keys())
    logger.info(f"Found {len(center_ids)} centers from {repr(chains_filepath)}")

    if chain_clusters_filepath is not None:
        chain_clusters = load_chain_cluster(chain_clusters_filepath)
        logger.info(f"Found {len(chain_clusters)} clusters from {repr(chain_clusters_filepath)}")
    else:
        chain_clusters = {x: [x] for x in center_ids}

    key_map = {}
    for key, vals in chain_clusters.items():
        for v in vals:
            key_map[v] = key

    parsed_chain_filepaths = []
    cluster_centers = []
    cluster_sizes = []
    for center_id in center_ids:
        center_id = _normalize_chain_id(center_id)
        try:
            center_id = key_map[center_id]
        except KeyError:
            logger.warning(f"Cannot find {center_id} in the clusters!")
            continue
        cluster_size = len(chain_clusters[center_id])
        for chain_id in chain_clusters[center_id]:
            pdb_id, asym_id = chain_id.split("_")
            pdb_id = pdb_id.lower()
            dv = pdb_id[1:3]
            parsed_chain_filepath = parsed_assembly_dirpath / dv / pdb_id / f"{pdb_id}_{asym_id}.pkl.gz"
            if parsed_assembly_dirpath.exists():
                parsed_chain_filepaths.append(parsed_chain_filepath)
                cluster_centers.append(center_id)
                cluster_sizes.append(cluster_size)
            else:
                logger.warning(f"Cannot find {repr(parsed_chain_filepath)}")

    logger.info(f"Processing (creating chain dicts)...")
    mmcif_chain_dicts = apply_func_parallel(
        func=create_chain_dict,
        args_list=[
            (parsed_chain_filepath, cluster_center, cluster_size)
            for parsed_chain_filepath, cluster_center, cluster_size in zip(parsed_chain_filepaths, cluster_centers, cluster_sizes)
        ],
        num_parallel_processes=num_parallel_processes,
    )
    mmcif_chains_df = pd.DataFrame.from_records(mmcif_chain_dicts)
    chains_filepath = output_dirpath / "chains.tsv"
    if not force:
        assert not chains_filepath.exists()
    mmcif_chains_df.to_csv(chains_filepath, index=False, sep="\t")
    logger.info(f"Chains saved to {repr(chains_filepath)} successfully!")

    logger.info("Copying PDB obsolete file...")
    src_pdb_obsolete_filepath = pdb_obsolete_filepath
    dst_pdb_obsolete_filepath = output_dirpath / pdb_obsolete_filepath.name
    if not force:
        assert not dst_pdb_obsolete_filepath.exists()
    shutil.copyfile(src=src_pdb_obsolete_filepath, dst=dst_pdb_obsolete_filepath)
    logger.info(f"PDB obsolete file copied to {repr(dst_pdb_obsolete_filepath)} successfully!")

    logger.info("process_gathered_chains finished successfully!")


def _normalize_chain_id(chain_id: str) -> str:
    pdb_id, asym_id = chain_id.split("_")
    pdb_id = pdb_id.lower()
    return f"{pdb_id}_{asym_id}"


def main() -> None:
    args = parse_args()
    setup_logging("gather.log")
    process_gathered_chains(
        parsed_assembly_dirpath=args.parsed_assembly_dirpath,
        output_dirpath=args.output_dirpath,
        chains_filepath=args.chains_filepath,
        pdb_obsolete_filepath=args.pdb_obsolete_filepath,
        num_parallel_processes=args.num_parallel_processes,
        force=args.force,
        chain_clusters_filepath=args.chain_clusters_filepath,
    )


if __name__ == "__main__":
    main()
